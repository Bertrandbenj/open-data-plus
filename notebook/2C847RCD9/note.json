{
  "paragraphs": [
    {
      "title": "Code",
      "text": "import scala.util.matching.Regex\nimport spark.implicits._\n\ncase class Quad(subject: String, predicate: String, objet: String, graph: String)\n\n\nvar originalPattern \u003d \n    \"^(\u003c[^\\\\s]+\u003e|_:(?:[A-Za-z][A-Za-z0-9\\\\-_]*))\\\\s+(\u003c[^\\\\s]+\u003e)\\\\s+(\u003c[^\\\\s]+\u003e|_:(?:[A-Za-z][A-Za-z0-9\\\\-_]*)|\\\\\\\"(?:(?:\\\\\\\"|[^\\\"])*)\\\\\\\"(?:@(?:[a-z]+[\\\\-A-Za-z0-9]*)|\\\\^\\\\^\u003c(?:[^\u003e]+)\u003e)?)\\\\s+(\u003c[^\\\\s]+\u003e).*$\".r\n\n\ndef buildQuad(s:String) : Quad \u003d {\n    \n    try{\n        originalPattern.findFirstMatchIn(s) match {\n            case Some(m) \u003d\u003e \n                if(m.group(1) \u003d\u003d null || m.group(2) \u003d\u003d null \n                || m.group(3) \u003d\u003d null || m.group(4) \u003d\u003d null )\n                    null\n                else\n                    Quad(m.group(1), m.group(2), m.group(3), m.group(4) )\n            case None \u003d\u003e \n                null\n        }\n    } catch {\n      case unknown : Throwable \u003d\u003e println(\"Exception parsing \" + unknown+\" \\n\")\n       null\n    }\n}\n\nval quads \u003d spark.read\t\n    .textFile(\"open-data-plus/results/m2/stdout_2\")\t\n\t.filter(_.length \u003c 1000)\n\t.map(buildQuad(_))\n\t.filter(q \u003d\u003e q !\u003d null)\n\nquads.show\n\t\nquads\n    //.filter(\"graph \u003c\u003e \\\"\\\" \")\n\t.withColumnRenamed(\"objet\",\"object\")            // Because .. ho well \n\t.write\n\t.mode(\"overwrite\")\n    //.bucketBy(20, \"graph\")\n\t//.sortBy(\"graph\", \"subject\", \"predicate\")\n    .parquet(\"m2\");\n\t\n\t",
      "dateUpdated": "Feb 6, 2017 1:29:38 AM",
      "config": {
        "colWidth": 6.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "tableHide": false,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1485369908610_-1976578595",
      "id": "20170125-194508_650590427",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\nimport scala.util.matching.Regex\n\nimport spark.implicits._\n\ndefined class Quad\n\noriginalPattern: scala.util.matching.Regex \u003d ^(\u003c[^\\s]+\u003e|_:(?:[A-Za-z][A-Za-z0-9\\-_]*))\\s+(\u003c[^\\s]+\u003e)\\s+(\u003c[^\\s]+\u003e|_:(?:[A-Za-z][A-Za-z0-9\\-_]*)|\\\"(?:(?:\\\"|[^\"])*)\\\"(?:@(?:[a-z]+[\\-A-Za-z0-9]*)|\\^\\^\u003c(?:[^\u003e]+)\u003e)?)\\s+(\u003c[^\\s]+\u003e).*$\n\nbuildQuad: (s: String)Quad\n\nquads: org.apache.spark.sql.Dataset[Quad] \u003d [subject: string, predicate: string ... 2 more fields]\n+-------+--------------------+--------------------+--------------------+\n|subject|           predicate|               objet|               graph|\n+-------+--------------------+--------------------+--------------------+\n|   _:b0|\u003chttp://www.w3.or...|\u003chttp://schema.or...|\u003chttp://www.first...|\n|   _:b0|\u003chttp://schema.or...|\"\"^^\u003chttp://www.w...|\u003chttp://www.first...|\n|   _:b0|\u003chttp://schema.or...|                _:b1|\u003chttp://www.first...|\n|   _:b0|\u003chttp://schema.or...|\u003chttp://www.first...|\u003chttp://www.first...|\n|   _:b1|\u003chttp://www.w3.or...|\u003chttp://schema.or...|\u003chttp://www.first...|\n|   _:b1|\u003chttp://schema.or...|\"required name\u003dse...|\u003chttp://www.first...|\n|   _:b1|\u003chttp://schema.or...|\"http://www.first...|\u003chttp://www.first...|\n|   _:b0|\u003chttp://www.w3.or...|\u003chttp://schema.or...|\u003chttp://sites.jcu...|\n|   _:b0|\u003chttp://schema.or...|\"John Carroll Uni...|\u003chttp://sites.jcu...|\n|   _:b0|\u003chttp://schema.or...|                _:b1|\u003chttp://sites.jcu...|\n|   _:b0|\u003chttp://schema.or...|\u003chttp://sites.jcu...|\u003chttp://sites.jcu...|\n|   _:b1|\u003chttp://www.w3.or...|\u003chttp://schema.or...|\u003chttp://sites.jcu...|\n|   _:b1|\u003chttp://schema.or...|\"required name\u003dse...|\u003chttp://sites.jcu...|\n|   _:b1|\u003chttp://schema.or...|\"http://sites.jcu...|\u003chttp://sites.jcu...|\n|   _:b0|\u003chttp://www.w3.or...|\u003chttp://schema.or...|\u003chttp://www.sport...|\n|   _:b0|\u003chttp://schema.or...|\"Sport 365 en dir...|\u003chttp://www.sport...|\n|   _:b0|\u003chttp://schema.or...|\u003chttp://www.sport...|\u003chttp://www.sport...|\n|   _:b0|\u003chttp://schema.or...|\"tennis, auto-mot...|\u003chttp://www.sport...|\n|   _:b0|\u003chttp://schema.or...|\"Sport365\"^^\u003chttp...|\u003chttp://www.sport...|\n|   _:b0|\u003chttp://schema.or...|                _:b1|\u003chttp://www.sport...|\n+-------+--------------------+--------------------+--------------------+\nonly showing top 20 rows\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norg.apache.spark.SparkException: Job aborted.\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:149)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:115)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:60)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:58)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:86)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:86)\n  at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:487)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:211)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:194)\n  at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:478)\n  ... 136 elided\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 46.0 failed 1 times, most recent failure: Lost task 2.0 in stage 46.0 (TID 304, localhost): org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.RuntimeException: Null value appeared in non-nullable field:\ntop level non-flat input object\nIf the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int).\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)\n\tat org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253)\n\tat org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252)\n\tat org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1325)\n\tat org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:258)\n\t... 8 more\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1904)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:143)\n  ... 155 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows\n  at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n  at org.apache.spark.scheduler.Task.run(Task.scala:85)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n  ... 3 more\nCaused by: java.lang.RuntimeException: Null value appeared in non-nullable field:\ntop level non-flat input object\nIf the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int).\n  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)\n  at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253)\n  at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252)\n  at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252)\n  at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1325)\n  at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:258)\n  ... 8 more\n"
      },
      "dateCreated": "Jan 25, 2017 7:45:08 AM",
      "dateStarted": "Feb 6, 2017 1:28:37 AM",
      "dateFinished": "Feb 6, 2017 1:28:47 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md \n# File conversion\n\nIn order to import the data into spark, we need to parse it and extract the structure from it. A first approach is to use a regular expression: \n\n```\n\u003c[^\\\\s]+\u003e                           // IRI \n_:(?:[A-Za-z][A-Za-z0-9\\\\-_]*)      // Blank Node \n\\\\\\\"(?:(?:\\\\\\\"|[^\\\"])*)\\\\\\\"         // String \n\n// strings can be followed by \n:@(?:[a-z]+[\\\\-A-Za-z0-9]*)         // lang tag \"a String\"@fr-FR\n\\\\^\\\\^\u003c(?:[^\u003e]+)\u003e                   // datatype ^^\u003cxsd:String\u003e\n```\nthe Regexp matches 4 entities\n\n```scala\n\n//(subject)    (predicate) (object)               (graph) .\n  ( IRI | BN ) ( IRI )     ( IRI | BN | String )  ( IRI )\n```\n\nthe column based parquet file fit the data pretty well as the compression level remains approximately the same. We can gain structure without wasting space on disk after parsing. the regexp being buggy on malformed input, we filter big lines (\u003e 2000 char). we loose some information there but this works on most Nquads and it is arguable whether we should analyze article bodies \n",
      "dateUpdated": "Feb 5, 2017 8:32:56 PM",
      "config": {
        "colWidth": 6.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1485624592353_1740922196",
      "id": "20170128-182952_1110062709",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch1\u003eFile conversion\u003c/h1\u003e\n\u003cp\u003eIn order to import the data into spark, we need to parse it and extract the structure from it. A first approach is to use a regular expression:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\u0026lt;[^\\\\s]+\u0026gt;                           // IRI \n_:(?:[A-Za-z][A-Za-z0-9\\\\-_]*)      // Blank Node \n\\\\\\\"(?:(?:\\\\\\\"|[^\\\"])*)\\\\\\\"         // String \n\n// strings can be followed by \n:@(?:[a-z]+[\\\\-A-Za-z0-9]*)         // lang tag \"a String\"@fr-FR\n\\\\^\\\\^\u0026lt;(?:[^\u0026gt;]+)\u0026gt;                   // datatype ^^\u0026lt;xsd:String\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ethe Regexp matches 4 entities\u003c/p\u003e\n\u003cpre\u003e\u003ccode class\u003d\"scala\"\u003e//(subject)    (predicate) (object)               (graph) .\n  ( IRI | BN ) ( IRI )     ( IRI | BN | String )  ( IRI )\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ethe column based parquet file fit the data pretty well as the compression level remains approximately the same. We can gain structure without wasting space on disk after parsing. the regexp being buggy on malformed input, we filter big lines (\u003e 2000 char). we loose some information there but this works on most Nquads and it is arguable whether we should analyze article bodies\u003c/p\u003e\n"
      },
      "dateCreated": "Jan 28, 2017 6:29:52 AM",
      "dateStarted": "Feb 5, 2017 8:32:48 PM",
      "dateFinished": "Feb 5, 2017 8:32:48 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sql \nSELECT * FROM m2",
      "dateUpdated": "Feb 6, 2017 1:16:39 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/sql"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1486318541461_1759619343",
      "id": "20170205-191541_1208708447",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "java.lang.ClassNotFoundException: $line365704135430.$read$\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.util.Utils$.classForName(Utils.scala:225)\n\tat org.apache.spark.sql.catalyst.encoders.OuterScopes$$anonfun$getOuterScope$1.apply(OuterScopes.scala:62)\n\tat org.apache.spark.sql.catalyst.expressions.objects.NewInstance$$anonfun$13.apply(objects.scala:238)\n\tat org.apache.spark.sql.catalyst.expressions.objects.NewInstance$$anonfun$13.apply(objects.scala:238)\n\tat scala.Option.map(Option.scala:146)\n\tat org.apache.spark.sql.catalyst.expressions.objects.NewInstance.doGenCode(objects.scala:238)\n\tat org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:104)\n\tat org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:101)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.objects.Invoke$$anonfun$6.apply(objects.scala:134)\n\tat org.apache.spark.sql.catalyst.expressions.objects.Invoke$$anonfun$6.apply(objects.scala:134)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:285)\n\tat org.apache.spark.sql.catalyst.expressions.objects.Invoke.doGenCode(objects.scala:134)\n\tat org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:104)\n\tat org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:101)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:101)\n\tat org.apache.spark.sql.execution.FilterExec.org$apache$spark$sql$execution$FilterExec$$genPredicate$1(basicPhysicalOperators.scala:127)\n\tat org.apache.spark.sql.execution.FilterExec$$anonfun$12.apply(basicPhysicalOperators.scala:169)\n\tat org.apache.spark.sql.execution.FilterExec$$anonfun$12.apply(basicPhysicalOperators.scala:153)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:285)\n\tat org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:153)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)\n\tat org.apache.spark.sql.execution.SerializeFromObjectExec.consume(objects.scala:99)\n\tat org.apache.spark.sql.execution.SerializeFromObjectExec.doConsume(objects.scala:119)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)\n\tat org.apache.spark.sql.execution.MapElementsExec.consume(objects.scala:190)\n\tat org.apache.spark.sql.execution.MapElementsExec.doConsume(objects.scala:217)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)\n\tat org.apache.spark.sql.execution.DeserializeToObjectExec.consume(objects.scala:66)\n\tat org.apache.spark.sql.execution.DeserializeToObjectExec.doConsume(objects.scala:84)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)\n\tat org.apache.spark.sql.execution.FilterExec.consume(basicPhysicalOperators.scala:79)\n\tat org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:194)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)\n\tat org.apache.spark.sql.execution.RowDataSourceScanExec.consume(ExistingRDD.scala:150)\n\tat org.apache.spark.sql.execution.RowDataSourceScanExec.doProduce(ExistingRDD.scala:217)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)\n\tat org.apache.spark.sql.execution.RowDataSourceScanExec.produce(ExistingRDD.scala:150)\n\tat org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:113)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)\n\tat org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:79)\n\tat org.apache.spark.sql.execution.DeserializeToObjectExec.doProduce(objects.scala:76)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)\n\tat org.apache.spark.sql.execution.DeserializeToObjectExec.produce(objects.scala:66)\n\tat org.apache.spark.sql.execution.MapElementsExec.doProduce(objects.scala:201)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)\n\tat org.apache.spark.sql.execution.MapElementsExec.produce(objects.scala:190)\n\tat org.apache.spark.sql.execution.SerializeFromObjectExec.doProduce(objects.scala:110)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)\n\tat org.apache.spark.sql.execution.SerializeFromObjectExec.produce(objects.scala:99)\n\tat org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:113)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)\n\tat org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:79)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:309)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:347)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:240)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:323)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:39)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2183)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2532)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2182)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2189)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1925)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1924)\n\tat org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2562)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:1924)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2139)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.zeppelin.spark.ZeppelinContext.showDF(ZeppelinContext.java:216)\n\tat org.apache.zeppelin.spark.SparkSqlInterpreter.interpret(SparkSqlInterpreter.java:129)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:94)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:341)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:176)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n"
      },
      "dateCreated": "Feb 5, 2017 7:15:41 PM",
      "dateStarted": "Feb 6, 2017 1:16:44 AM",
      "dateFinished": "Feb 6, 2017 1:16:44 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "spark.sql(\"SELECT * FROM micro3\")\n    .write\n\t.mode(\"overwrite\")\n    .bucketBy(10, \"graph\")\n\t.sortBy(\"graph\")\n\t.saveAsTable(\"micro3sorted\");",
      "dateUpdated": "Feb 5, 2017 7:33:57 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1485878721427_-514297317",
      "id": "20170131-170521_89137319",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norg.apache.spark.SparkException: Job aborted.\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:149)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:115)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:60)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:58)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:86)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:86)\n  at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:487)\n  at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:246)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:60)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:58)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:86)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:86)\n  at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:378)\n  at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:354)\n  ... 75 elided\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 89.0 failed 1 times, most recent failure: Lost task 3.0 in stage 89.0 (TID 1767, localhost): org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer.writeRows(WriterContainer.scala:446)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.io.ByteArrayOutputStream.\u003cinit\u003e(ByteArrayOutputStream.java:77)\n\tat org.apache.parquet.bytes.BytesInput$BAOS.\u003cinit\u003e(BytesInput.java:175)\n\tat org.apache.parquet.bytes.BytesInput$BAOS.\u003cinit\u003e(BytesInput.java:173)\n\tat org.apache.parquet.bytes.BytesInput.toByteArray(BytesInput.java:161)\n\tat org.apache.parquet.bytes.ConcatenatingByteArrayCollector.collect(ConcatenatingByteArrayCollector.java:33)\n\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:113)\n\tat org.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:152)\n\tat org.apache.parquet.column.impl.ColumnWriterV1.accountForValueWritten(ColumnWriterV1.java:113)\n\tat org.apache.parquet.column.impl.ColumnWriterV1.write(ColumnWriterV1.java:205)\n\tat org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.addBinary(MessageColumnIO.java:346)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$anonfun$org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter$8.apply(ParquetWriteSupport.scala:153)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$anonfun$org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter$8.apply(ParquetWriteSupport.scala:152)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$anonfun$org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$writeFields$1.apply$mcV$sp(ParquetWriteSupport.scala:114)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$consumeField(ParquetWriteSupport.scala:421)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$writeFields(ParquetWriteSupport.scala:113)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$anonfun$write$1.apply$mcV$sp(ParquetWriteSupport.scala:104)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:409)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:103)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:51)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:121)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:123)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:42)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.writeInternal(ParquetFileFormat.scala:553)\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer$$anonfun$writeRows$4.apply$mcV$sp(WriterContainer.scala:430)\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer$$anonfun$writeRows$4.apply(WriterContainer.scala:416)\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer$$anonfun$writeRows$4.apply(WriterContainer.scala:416)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1325)\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer.writeRows(WriterContainer.scala:438)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGSchedul\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ner.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1904)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:143)\n  ... 105 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows\n  at org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer.writeRows(WriterContainer.scala:446)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n  at org.apache.spark.scheduler.Task.run(Task.scala:85)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n  ... 3 more\nCaused by: java.lang.OutOfMemoryError: Java heap space\n  at java.io.ByteArrayOutputStream.\u003cinit\u003e(ByteArrayOutputStream.java:77)\n  at org.apache.parquet.bytes.BytesInput$BAOS.\u003cinit\u003e(BytesInput.java:175)\n  at org.apache.parquet.bytes.BytesInput$BAOS.\u003cinit\u003e(BytesInput.java:173)\n  at org.apache.parquet.bytes.BytesInput.toByteArray(BytesInput.java:161)\n  at org.apache.parquet.bytes.ConcatenatingByteArrayCollector.collect(ConcatenatingByteArrayCollector.java:33)\n  at org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writePage(ColumnChunkPageWriteStore.java:113)\n  at org.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:152)\n  at org.apache.parquet.column.impl.ColumnWriterV1.accountForValueWritten(ColumnWriterV1.java:113)\n  at org.apache.parquet.column.impl.ColumnWriterV1.write(ColumnWriterV1.java:205)\n  at org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.addBinary(MessageColumnIO.java:346)\n  at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$anonfun$org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter$8.apply(ParquetWriteSupport.scala:153)\n  at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$anonfun$org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter$8.apply(ParquetWriteSupport.scala:152)\n  at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$anonfun$org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$writeFields$1.apply$mcV$sp(ParquetWriteSupport.scala:114)\n  at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$consumeField(ParquetWriteSupport.scala:421)\n  at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$writeFields(ParquetWriteSupport.scala:113)\n  at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$$anonfun$write$1.apply$mcV$sp(ParquetWriteSupport.scala:104)\n  at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:409)\n  at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:103)\n  at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:51)\n  at org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:121)\n  at org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:123)\n  at org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:42)\n  at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.writeInternal(ParquetFileFormat.scala:553)\n  at org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer$$anonfun$writeRows$4.apply$mcV$sp(WriterContainer.scala:430)\n  at org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer$$anonfun$writeRows$4.apply(WriterContainer.scala:416)\n  at org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer$$anonfun$writeRows$4.apply(WriterContainer.scala:416)\n  at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1325)\n  at org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer.writeRows(WriterContainer.scala:438)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n  at org.apache.spark.scheduler.Task.run(Task.scala:85)\n"
      },
      "dateCreated": "Jan 31, 2017 5:05:21 AM",
      "dateStarted": "Feb 1, 2017 12:26:31 PM",
      "dateFinished": "Feb 1, 2017 12:28:00 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val uri \u003d \"\"\"(\u003c[^\\s]+\u003e)\\s+\\.$\"\"\".r\nval urlToTest \u003d spark.read.textFile(\"urlToTest\").orderBy(\"value\").collect\n\n    spark.read.textFile(\"open-data-plus/dump/daily2/*gz\")\n        .filter( line \u003d\u003e uri.findFirstMatchIn(line) match {\n                case Some(m) \u003d\u003e\n                    urlToTest.contains(m.group(1))\n                case None \u003d\u003e\n                    false\n            })\n        .repartition(1)\n        .write\n        .mode(\"append\")\n        .text(\"ofInterest.gz\")",
      "dateUpdated": "Feb 4, 2017 1:36:13 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1485638575130_1508510764",
      "id": "20170128-222255_1845986177",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\nuri: scala.util.matching.Regex \u003d (\u003c[^\\s]+\u003e)\\s+\\.$\nurlToTest: Array[String] \u003d Array(\u003chttp://16septembre1943.blogspot.fr/\u003e, \u003chttp://22rue.canalblog.com/archives/2012/08/19/24922402.html\u003e, \u003chttp://22rue.canalblog.com/archives/2012/11/01/25469657.html\u003e, \u003chttp://22rue.canalblog.com/archives/2013/02/25/26493996.html\u003e, \u003chttp://22rue.canalblog.com/archives/2013/11/17/28448939.html\u003e, \u003chttp://44.agendaculturel.fr/\u003e, \u003chttp://44.agendaculturel.fr/cite-des-congres-de-nantes\u003e, \u003chttp://44.agendaculturel.fr/concert/nantes/\u003e, \u003chttp://44.agendaculturel.fr/concert/nantes/the-residents.html\u003e, \u003chttp://44.agendaculturel.fr/concert/nantes/vaguement-la-jungle.html\u003e, \u003chttp://44.agendaculturel.fr/le-grand-t\u003e, \u003chttp://4ad.com/live/11796\u003e, \u003chttp://4ad.com/live/13141\u003e, \u003chttp://4c.ucc.ie/cpaior2014/\u003e, \u003chttp://4design.xyz/tutoriels-seo-liens-referencement\u003e, \u003chttp://...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norg.apache.spark.SparkException: Job aborted.\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:149)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:115)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:60)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:58)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:86)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:86)\n  at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:487)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:211)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:194)\n  at org.apache.spark.sql.DataFrameWriter.text(DataFrameWriter.scala:520)\n  ... 55 elided\nCaused by: org.apache.spark.SparkException: Job 32 cancelled part of cancelled job group zeppelin-20170128-222255_1845986177\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1389)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:795)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:795)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:795)\n  at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:795)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1638)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1904)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:143)\n  ... 74 more\n"
      },
      "dateCreated": "Jan 28, 2017 10:22:55 AM",
      "dateStarted": "Jan 30, 2017 1:08:08 AM",
      "dateFinished": "Jan 30, 2017 1:54:59 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "case class Quad(subject: String, predicate: String, objet: String, graph: String)\n\nspark.read.textFile(\"open-data-plus/dump/dpef.html-microdata.nq-00003.gz\")\n    .filter(_.contains(\"Nantes\"))\n    .map(graph.findFirstMatchIn(_) match {\n            case Some(m) \u003d\u003e\n                m.group(1) \n            case None \u003d\u003e\n                println(\"Problem parsing \")\n                null\n        })\n    .distinct\n    .repartition(1)\n    .show(false)\n    //.write\n    //.mode(\"overwrite\")\n    //.text(\"urlToTest\")\n\n    \n",
      "dateUpdated": "Jan 29, 2017 12:15:01 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1485626975665_-395402388",
      "id": "20170128-190935_1034871736",
      "dateCreated": "Jan 28, 2017 7:09:35 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n## Another approach \n\nParse NQuads and build [Jena objects](http://jena.apache.org/) to leverage the software stack at our disposal. Namely a SPARQL engine, Ontologies, reasoning, ...\n\n## A specific structure\n\nIt wouldnt be a smart idea to build a single Jena graph out of a terabyte of compressed Quads : it wouldn\u0027t fit in memory and even if it is eventually possible, the computing time would remain tied to monothreaded concepts. \n\nWe benefit from a dataset that is \n\n* very large (1,7 bn distinct URLs) \n* but quite flat ( 40 predicate/URL average) \n\nMeaning we can split the dataset and build a Jena graph for each of these webpage. \n\nIndeed the fact those triple are foud on webpage indicate us, it will never be so big that one single machine cant handle it. perhaps some graph will be bigger than other but we consider reasonable the assumption: each URL\u0027s graph can hold in one machine and be processed by a single Thread. \n\nWe then consider Spark a simple triple store of many many small graphs. \n\nby disconnecting the graphs we ensure a decent parallelization but omit the links between the webpages (hyperlink \u0026 url predicates) \n\nMore importantly, we need to remember the structure around those hyperlinks, and urls predicates that tells us wether or not the URL is of quality for the topic that interests us. \n\nHowever and because it may help in the analysis of our data, we show it is possible to obtain Jena objects using a technique inspired by [SANSA-Stack](https://github.com/SANSA-Stack)\n",
      "dateUpdated": "Feb 5, 2017 8:09:01 PM",
      "config": {
        "colWidth": 6.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1486319305770_894978530",
      "id": "20170205-192825_765143907",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch2\u003eAnother approach\u003c/h2\u003e\n\u003cp\u003eParse NQuads and build \u003ca href\u003d\"http://jena.apache.org/\"\u003eJena objects\u003c/a\u003e to leverage the software stack at our disposal. Namely a SPARQL engine, Ontologies, reasoning, \u0026hellip;\u003c/p\u003e\n\u003ch2\u003eA specific structure\u003c/h2\u003e\n\u003cp\u003eIt wouldnt be a smart idea to build a single Jena graph out of a terabyte of compressed Quads : it wouldn\u0027t fit in memory and even if it is eventually possible, the computing time would remain tied to monothreaded concepts.\u003c/p\u003e\n\u003cp\u003eWe benefit from a dataset that is\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003every large (1,7 bn distinct URLs)\u003c/li\u003e\n\u003cli\u003ebut quite flat ( 40 predicate/URL average)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eMeaning we can split the dataset and build a Jena graph for each of these webpage.\u003c/p\u003e\n\u003cp\u003eIndeed the fact those triple are foud on webpage indicate us, it will never be so big that one single machine cant handle it. perhaps some graph will be bigger than other but we consider reasonable the assumption: each URL\u0027s graph can hold in one machine and be processed by a single Thread.\u003c/p\u003e\n\u003cp\u003eWe then consider Spark a simple triple store of many many small graphs.\u003c/p\u003e\n\u003cp\u003eby disconnecting the graphs we ensure a decent parallelization but omit the links between the webpages (hyperlink \u0026amp; url predicates)\u003c/p\u003e\n\u003cp\u003eMore importantly, we need to remember the structure around those hyperlinks, and urls predicates that tells us wether or not the URL is of quality for the topic that interests us.\u003c/p\u003e\n\u003cp\u003eHowever and because it may help in the analysis of our data, we show it is possible to obtain Jena objects using a technique inspired by \u003ca href\u003d\"https://github.com/SANSA-Stack\"\u003eSANSA-Stack\u003c/a\u003e\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 5, 2017 7:28:25 PM",
      "dateStarted": "Feb 5, 2017 8:08:54 PM",
      "dateFinished": "Feb 5, 2017 8:08:54 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Imports and Functions",
      "text": "import scala.util.matching.Regex\nimport java.io.File\nimport java.io.ByteArrayInputStream\nimport spark.implicits._\n\nimport org.apache.jena.riot.lang.PipedRDFIterator\nimport org.apache.jena.riot.lang.PipedQuadsStream\nimport org.apache.jena.riot.RDFDataMgr\nimport org.apache.jena.riot.Lang\nimport org.apache.jena.sparql.core.Quad\nimport org.apache.jena.rdf.model.Model\nimport org.apache.jena.rdf.model.ModelFactory\nimport org.apache.jena.graph.Factory\nimport org.apache.jena.rdf.model.impl.StatementImpl\n\nimplicit val EncQuad \u003d org.apache.spark.sql.Encoders.kryo[org.apache.jena.sparql.core.Quad]\nimplicit val EncTriple \u003d org.apache.spark.sql.Encoders.kryo[org.apache.jena.graph.Triple]\nimplicit val EncNode \u003d org.apache.spark.sql.Encoders.kryo[org.apache.jena.graph.Node]\nimplicit val EncModel \u003d org.apache.spark.sql.Encoders.kryo[org.apache.jena.rdf.model.Model]\nimplicit val EncGraph \u003d org.apache.spark.sql.Encoders.kryo[org.apache.jena.graph.Graph]\n\n//import net.sansa_stack.rdf.spark.io.JenaKryoSerializers\n\n\n\ndef jenaQuad(s:String) : org.apache.jena.sparql.core.Quad \u003d {\n    try {\n        val it2 \u003d new PipedRDFIterator[org.apache.jena.sparql.core.Quad]\n        \n        RDFDataMgr.parse(\n        \tnew PipedQuadsStream(it2), \n        \tnew ByteArrayInputStream(s.getBytes), \n        \tnull, \n        \tLang.NQUADS)\n        it2.next()\n        \n    }  catch  {\n      case unknown : Throwable \u003d\u003e println(\"exception: \" + unknown)\n      null\n    }\n}\n\ndef jenaModels(quads: Iterator[org.apache.jena.sparql.core.Quad]) : org.apache.jena.graph.Graph \u003d {\n    try {\n        println(\"sdf\")\n        val model \u003d ModelFactory.createOntologyModel()\n        val graph \u003d Factory.createDefaultGraph();\n        quads.toList\n            .map(_.asTriple)\n            .foreach(graph.add(_))\n        println(graph)\n        graph\n        \n    }  catch {\n      case unknown : Throwable \u003d\u003e println(\"   jenaModels  exception: \" + unknown)\n      null\n    }\n}\n\n",
      "dateUpdated": "Feb 5, 2017 8:08:07 PM",
      "config": {
        "colWidth": 6.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1486165957617_-1452561108",
      "id": "20170204-005237_986887158",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nimport scala.util.matching.Regex\n\nimport java.io.File\n\nimport net.sansa_stack.rdf.spark.io.NTripleReader\n\nimport org.apache.jena.riot.lang.PipedRDFIterator\n\nimport org.apache.jena.riot.lang.PipedQuadsStream\n\nimport org.apache.jena.riot.RDFDataMgr\n\nimport org.apache.jena.riot.Lang\n\nimport java.io.ByteArrayInputStream\n\nimport org.apache.jena.sparql.core.Quad\n\nmyObjEncoder: org.apache.spark.sql.Encoder[org.apache.jena.sparql.core.Quad] \u003d class[value[0]: binary]\n\nmyObjEncoder2: org.apache.spark.sql.Encoder[org.apache.jena.graph.Triple] \u003d class[value[0]: binary]\n\nimport spark.implicits._\n\nrdd: org.apache.spark.sql.Dataset[String] \u003d [value: string]\n\njenaQuad: (s: String)org.apache.jena.sparql.core.Quad\n\nquad: org.apache.spark.sql.Dataset[org.apache.jena.sparql.core.Quad] \u003d [value: binary]\n+--------------------+\n|               value|\n+--------------------+\n|[http://1079theli...|\n|[http://1079theli...|\n|[http://1079theli...|\n|[http://1079theli...|\n|[http://1079theli...|\n|[http://11198-if-...|\n|[http://11198-if-...|\n|[http://11198-if-...|\n|[http://11198-if-...|\n|[http://18wosmexi...|\n|[http://18wosmexi...|\n|[http://18wosmexi...|\n|[http://18wosmexi...|\n|[http://67.agenda...|\n|[http://67.agenda...|\n|[http://67.agenda...|\n|[http://67.agenda...|\n|[http://67.agenda...|\n|[http://67.agenda...|\n|[http://67.agenda...|\n+--------------------+\nonly showing top 20 rows\n\n[http://1079thelink.com/2014/04/17/fame-files-tom-cruise-secretly-dating-fellow-scientologist-laura-prepon/ 03fc4ab0c585b5281b50fca5f232eca4 http://www.w3.org/1999/02/22-rdf-syntax-ns#type http://schema.org/BlogPosting]\n[http://1079thelink.com/2014/04/17/fame-files-tom-cruise-secretly-dating-fellow-scientologist-laura-prepon/ df1fb99a05d4046c693d725d7217daa5 http://schema.org/BlogPosting/articleBody \"\n\n\t\t\t\t\tvar livefyre_collection_data \u003d {\\\"post_meta\\\":{\\\"article_id\\\":\\\"203015\\\",\\\"article_path\\\":\\\"\\/2014\\/04\\/17\\/fame-files-tom-cruise-secretly-dating-fellow-scientologist-laura-prepon\\/\\\",\\\"article_title\\\":\\\"Fame Files: \\t Tom Cruise Secretly Dating Fellow Scientologist Laura Prepon\\\",\\\"read_only\\\":false},\\\"tokens\\\":{\\\"collection_meta\\\":\\\"eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ0eXBlIjoibGl2ZWNvbW1lbnRzIiwiYXJ0aWNsZUlkIjoiMjAzMDE1IiwidGl0bGUiOiJGYW1lIEZpbGVzOiBcdCBUb20gQ3J1aXNlIFNlY3JldGx5IERhdGluZyBGZWxsb3cgU2NpZW50b2xvZ2lzdCBMYXVyYSBQcmVwb24iLCJ1cmwiOiJodHRwOlwvXC8xMDc5dGhlbGluay5jb21cLzIwMTRcLzA0XC8xN1wvZmFtZS1maWxlcy10b20tY3J1aXNlLXNlY3JldGx5LWRhdGluZy1mZWxsb3ctc2NpZW50b2xvZ2lzdC1sYXVyYS1wcmVwb25cLyIsImlzcyI6InVybjpsaXZlZnlyZTpnbWNoYXJsb3R0ZS5meXJlLmNvOnNpdGU9MzY0NTU5In0.PQXgD31cAgSpZwxlcRKCSwoPkP--Hio7jScxeTPYwrE\\\",\\\"checksum\\\":\\\"24b4ae23bb7730ef9075891b7ecf8bbd\\\"}};Is Prepon the new Holmes? According to a new report from the New York Post\u0027s Page Six, Tom Cruise has been secretly dating Orange Is the New Black star Laura Preponfor several months.\nRead more: http://www.usmagazine.com/celebrity-news/news/tom-cruise-secretly-dating-laura-prepon-2014174#ixzz2zBC9fo00\n\n\t\t\t\t\"^^http://www.w3.org/2001/XMLSchema#string]\n[http://1079thelink.com/2014/04/17/fame-files-tom-cruise-secretly-dating-fellow-scientologist-laura-prepon/ c46f7704f4abcafbd501939540b8bda5 http://schema.org/BlogPosting/headline \"Fame Files: \t Tom Cruise Secretly Dating Fellow Scientologist Laura Prepon\"^^http://www.w3.org/2001/XMLSchema#string]\n[http://1079thelink.com/2014/04/17/fame-files-tom-cruise-secretly-dating-fellow-scientologist-laura-prepon/ http://1079thelink.com/2014/04/17/fame-files-tom-cruise-secretly-dating-fellow-scientologist-laura-prepon/ http://www.w3.org/1999/xhtml/microdata#item 509299349014644697a7b8af492c8ee7]\n[http://1079thelink.com/2014/04/17/fame-files-tom-cruise-secretly-dating-fellow-scientologist-laura-prepon/ http://1079thelink.com/2014/04/17/fame-files-tom-cruise-secretly-dating-fellow-scientologist-laura-prepon/ http://purl.org/dc/terms/title \"Fame Files:  Tom Cruise Secretly Dating Fellow Scientologist Laura Prepon - 107.9 The Link\"^^http://www.w3.org/2001/XMLSchema#string]\n[http://11198-if-unsika.blogspot.com/ 4aa98e5faf0c27505b0cfa3ed9460af4 http://www.w3.org/1999/02/22-rdf-syntax-ns#type http://schema.org/Blog]\n[http://11198-if-unsika.blogspot.com/ 88c25d0618a78c7ad470d0071d0d5edf http://schema.org/Blog/name \"Irfan Blog\u0027s\"^^http://www.w3.org/2001/XMLSchema#string]\n[http://11198-if-unsika.blogspot.com/ http://11198-if-unsika.blogspot.com/ http://www.w3.org/1999/xhtml/microdata#item 8af447820d80aead8eb050d5a2c0f62c]\n[http://11198-if-unsika.blogspot.com/ http://11198-if-unsika.blogspot.com/ http://purl.org/dc/terms/title \"Irfan Blog\u0027s\"^^http://www.w3.org/2001/XMLSchema#string]\n[http://18wosmexican.mexico-foro.com/t283-pack-de-camiones-kenworth-t800-by-maluco 136d8665761e767dd5b0ce36e7c1d12e http://www.w3.org/1999/02/22-rdf-syntax-ns#type http://data-vocabulary.org/Breadcrumb]\n"
      },
      "dateCreated": "Feb 4, 2017 12:52:37 PM",
      "dateStarted": "Feb 4, 2017 7:48:33 AM",
      "dateFinished": "Feb 4, 2017 7:48:39 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "quad.map(_.getPredicate.toString).show(false)\nquad.filter(_.getGraph \u003d\u003d null ).show\nval keyed \u003d quad.groupByKey(_.getGraph.toString)\nkeyed.keys.show\nval kc \u003d keyed.count\nval keyedToTriple \u003d keyed.mapGroups((n,qList) \u003d\u003e jenaModels(qList))\nkeyedToTriple.count",
      "dateUpdated": "Feb 5, 2017 7:28:10 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1486167282075_1098637161",
      "id": "20170204-011442_332931371",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\nNodeEnc: org.apache.spark.sql.Encoder[org.apache.jena.graph.Node] \u003d class[value[0]: binary]\n\nModelEnc: org.apache.spark.sql.Encoder[org.apache.jena.rdf.model.Model] \u003d class[value[0]: binary]\n\nGraphEnc: org.apache.spark.sql.Encoder[org.apache.jena.graph.Graph] \u003d class[value[0]: binary]\n\nimport org.apache.jena.rdf.model.Model\n\nimport org.apache.jena.rdf.model.ModelFactory\n\nimport org.apache.jena.graph.Factory\n\nimport org.apache.jena.rdf.model.impl.StatementImpl\n\njenaModels: (quads: Iterator[org.apache.jena.sparql.core.Quad])org.apache.jena.graph.Graph\n+-----------------------------------------------+\n|value                                          |\n+-----------------------------------------------+\n|http://www.w3.org/1999/02/22-rdf-syntax-ns#type|\n|http://schema.org/BlogPosting/articleBody      |\n|http://schema.org/BlogPosting/headline         |\n|http://www.w3.org/1999/xhtml/microdata#item    |\n|http://purl.org/dc/terms/title                 |\n|http://www.w3.org/1999/02/22-rdf-syntax-ns#type|\n|http://schema.org/Blog/name                    |\n|http://www.w3.org/1999/xhtml/microdata#item    |\n|http://purl.org/dc/terms/title                 |\n|http://www.w3.org/1999/02/22-rdf-syntax-ns#type|\n|http://data-vocabulary.org/Breadcrumb/title    |\n|http://data-vocabulary.org/Breadcrumb/url      |\n|http://www.w3.org/1999/xhtml/microdata#item    |\n|http://www.w3.org/1999/02/22-rdf-syntax-ns#type|\n|http://data-vocabulary.org/Breadcrumb/title    |\n|http://data-vocabulary.org/Breadcrumb/url      |\n|http://www.w3.org/1999/xhtml/microdata#item    |\n|http://www.w3.org/1999/02/22-rdf-syntax-ns#type|\n|http://data-vocabulary.org/Breadcrumb/title    |\n|http://data-vocabulary.org/Breadcrumb/url      |\n+-----------------------------------------------+\nonly showing top 20 rows\n\n+-----+\n|value|\n+-----+\n+-----+\n\n\nkeyed: org.apache.spark.sql.KeyValueGroupedDataset[String,org.apache.jena.sparql.core.Quad] \u003d org.apache.spark.sql.KeyValueGroupedDataset@6513ce8f\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 50.0 failed 1 times, most recent failure: Lost task 0.0 in stage 50.0 (TID 36, localhost): java.lang.NullPointerException\n\tat $anonfun$1.apply(\u003cconsole\u003e:209)\n\tat $anonfun$1.apply(\u003cconsole\u003e:209)\n\tat org.apache.spark.sql.execution.AppendColumnsWithObjectExec$$anonfun$10$$anonfun$apply$3.apply(objects.scala:287)\n\tat org.apache.spark.sql.execution.AppendColumnsWithObjectExec$$anonfun$10$$anonfun$apply$3.apply(objects.scala:285)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\n  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:347)\n  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:39)\n  at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2183)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n  at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2532)\n  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2182)\n  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2189)\n  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1925)\n  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1924)\n  at org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2562)\n  at org.apache.spark.sql.Dataset.head(Dataset.scala:1924)\n  at org.apache.spark.sql.Dataset.take(Dataset.scala:2139)\n  at org.apache.spark.sql.Dataset.showString(Dataset.scala:239)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:526)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:486)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:495)\n  ... 60 elided\nCaused by: java.lang.NullPointerException\n  at $anonfun$1.apply(\u003cconsole\u003e:209)\n  at $anonfun$1.apply(\u003cconsole\u003e:209)\n  at org.apache.spark.sql.execution.AppendColumnsWithObjectExec$$anonfun$10$$anonfun$apply$3.apply(objects.scala:287)\n  at org.apache.spark.sql.execution.AppendColumnsWithObjectExec$$anonfun$10$$anonfun$apply$3.apply(objects.scala:285)\n  at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown Source)\n  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n  at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\n  at org.apache.spark.scheduler.Task.run(Task.scala:85)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n  ... 3 more\n"
      },
      "dateCreated": "Feb 4, 2017 1:14:42 AM",
      "dateStarted": "Feb 4, 2017 8:54:57 AM",
      "dateFinished": "Feb 4, 2017 9:00:03 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "keyed.mapGroups( (g, q) \u003d\u003e { \n    val graph \u003d Factory.createDefaultGraph(); \n    q.map(_.asTriple).foreach(graph.add(_)); \n    graph} )\n    .map(_.toString).show",
      "dateUpdated": "Feb 4, 2017 8:22:35 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1486235566841_226875401",
      "id": "20170204-201246_12461377",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 34.0 failed 1 times, most recent failure: Lost task 0.0 in stage 34.0 (TID 24, localhost): java.lang.NullPointerException\n\tat $anonfun$1.apply(\u003cconsole\u003e:185)\n\tat $anonfun$1.apply(\u003cconsole\u003e:185)\n\tat org.apache.spark.sql.execution.AppendColumnsWithObjectExec$$anonfun$10$$anonfun$apply$3.apply(objects.scala:287)\n\tat org.apache.spark.sql.execution.AppendColumnsWithObjectExec$$anonfun$10$$anonfun$apply$3.apply(objects.scala:285)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:148)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\n  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:347)\n  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:39)\n  at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2183)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n  at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2532)\n  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2182)\n  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2189)\n  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1925)\n  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1924)\n  at org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2562)\n  at org.apache.spark.sql.Dataset.head(Dataset.scala:1924)\n  at org.apache.spark.sql.Dataset.take(Dataset.scala:2139)\n  at org.apache.spark.sql.Dataset.showString(Dataset.scala:239)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:526)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:486)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:495)\n  ... 60 elided\nCaused by: java.lang.NullPointerException\n  at $anonfun$1.apply(\u003cconsole\u003e:185)\n  at $anonfun$1.apply(\u003cconsole\u003e:185)\n  at org.apache.spark.sql.execution.AppendColumnsWithObjectExec$$anonfun$10$$anonfun$apply$3.apply(objects.scala:287)\n  at org.apache.spark.sql.execution.AppendColumnsWithObjectExec$$anonfun$10$$anonfun$apply$3.apply(objects.scala:285)\n  at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n  at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n  at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:148)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\n  at org.apache.spark.scheduler.Task.run(Task.scala:85)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n  ... 3 more\n"
      },
      "dateCreated": "Feb 4, 2017 8:12:46 AM",
      "dateStarted": "Feb 4, 2017 8:22:35 AM",
      "dateFinished": "Feb 4, 2017 8:22:37 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "print(s\"\"\"%angular\n\u003cscript\u003e\nvar firstScript \u003d document.getElementsByTagName(\u0027script\u0027)[0],\n      js \u003d document.createElement(\u0027script\u0027);\n  js.src \u003d \u0027https://cdnjs.cloudflare.com/ajax/libs/Snowstorm/20131208/snowstorm-min.js\u0027;\n  js.onload \u003d function () {\n    // do stuff with your dynamically loaded script\n    snowStorm.snowColor \u003d \u0027#99ccff\u0027;\n  };\n  firstScript.parentNode.insertBefore(js, firstScript);\n\n\u003c/script\u003e\n\n\"\"\")\n\nprint(s\"\"\"%html\n\u003cscript\u003e\n    snowStorm.start();\n\u003c/script\u003e\n\n\"\"\")\n\nprint(s\"\"\"%html\n\u003cscript\u003e\n\nvar firstScript \u003d document.getElementsByTagName(\u0027script\u0027)[0],\n      js \u003d document.createElement(\u0027script\u0027);\n  js.src \u003d \u0027https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.22/require.js\u0027;\n  firstScript.parentNode.insertBefore(js, firstScript);\n\n\u003c/script\u003e\n\n\"\"\")\n\nprint(s\"\"\"%html\n\u003cdiv id\u003d\"chart_placeholder\"\u003e\u003c/div\u003e\n\u003cstyle\u003e\n.zoom{\n    fill-opacity:0;\n}\n\u003c/style\u003e\n\u003cscript\u003e\n\nrequire.config({\n    paths: {\n        \u0027eventDrops\u0027 : \u0027https://cdn.rawgit.com/marmelab/EventDrops/d55f8b001dc659eacef20edfec8b7159cffaa923/dist/eventDrops\u0027 // keep this at a version... not very nice but what can you do\n    }\n});\n\nrequirejs([\"eventDrops\"], function(eD) {\n    var data \u003d [];\n    var names \u003d [\"Lorem\", \"Ipsum\", \"Dolor\", \"Sit\", \"Amet\", \"Consectetur\", \"Adipisicing\", \"elit\", \"Eiusmod tempor\", \"Incididunt\"];\n    var endTime \u003d Date.now();\n    var month \u003d 30 * 24 * 60 * 60 * 1000;\n    var startTime \u003d endTime - 6 * month;\n\n    function createEvent (name, maxNbEvents) {\n        maxNbEvents \u003d maxNbEvents | 200;\n        var event \u003d {\n            name: name,\n            dates: []\n        };\n        // add up to 200 events\n        var max \u003d  Math.floor(Math.random() * maxNbEvents);\n        for (var j \u003d 0; j \u003c max; j++) {\n            var time \u003d (Math.random() * (endTime - startTime)) + startTime;\n            event.dates.push(new Date(time));\n        }\n        return event;\n    }\n\n    for (var i \u003d 0; i \u003c 10; i++) {\n        data.push(createEvent(names[i]));\n    }\n\n    var color \u003d d3.scale.category20();\n\n    var eventDropsChart \u003d d3.chart.eventDrops()\n        .eventLineColor(function (datum, index) {\n            return color(index);\n        })\n        .start(new Date(startTime))\n        .end(new Date(endTime))\n        .hasTopAxis(false)\n        .hasBottomAxis(false);\n\n    d3.select(\u0027#chart_placeholder\u0027)\n      .datum(data)\n      .call(eventDropsChart);\n\n});\n\u003c/script\u003e\n\"\"\")\n",
      "dateUpdated": "Feb 4, 2017 12:52:35 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "tableHide": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1485369905913_1593906523",
      "id": "20170125-194505_441348329",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cscript\u003e\nvar firstScript \u003d document.getElementsByTagName(\u0027script\u0027)[0],\n      js \u003d document.createElement(\u0027script\u0027);\n  js.src \u003d \u0027https://cdnjs.cloudflare.com/ajax/libs/Snowstorm/20131208/snowstorm-min.js\u0027;\n  js.onload \u003d function () {\n    // do stuff with your dynamically loaded script\n    snowStorm.snowColor \u003d \u0027#99ccff\u0027;\n  };\n  firstScript.parentNode.insertBefore(js, firstScript);\n\n\u003c/script\u003e\n\n\u003cscript\u003e\n    snowStorm.start();\n\u003c/script\u003e\n\n\u003cscript\u003e\n\nvar firstScript \u003d document.getElementsByTagName(\u0027script\u0027)[0],\n      js \u003d document.createElement(\u0027script\u0027);\n  js.src \u003d \u0027https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.22/require.js\u0027;\n  firstScript.parentNode.insertBefore(js, firstScript);\n\n\u003c/script\u003e\n\n\u003cdiv id\u003d\"chart_placeholder\"\u003e\u003c/div\u003e\n\u003cstyle\u003e\n.zoom{\n    fill-opacity:0;\n}\n\u003c/style\u003e\n\u003cscript\u003e\n\nrequire.config({\n    paths: {\n        \u0027eventDrops\u0027 : \u0027https://cdn.rawgit.com/marmelab/EventDrops/d55f8b001dc659eacef20edfec8b7159cffaa923/dist/eventDrops\u0027 // keep this at a version... not very nice but what can you do\n    }\n});\n\nrequirejs([\"eventDrops\"], function(eD) {\n    var data \u003d [];\n    var names \u003d [\"Lorem\", \"Ipsum\", \"Dolor\", \"Sit\", \"Amet\", \"Consectetur\", \"Adipisicing\", \"elit\", \"Eiusmod tempor\", \"Incididunt\"];\n    var endTime \u003d Date.now();\n    var month \u003d 30 * 24 * 60 * 60 * 1000;\n    var startTime \u003d endTime - 6 * month;\n\n    function createEvent (name, maxNbEvents) {\n        maxNbEvents \u003d maxNbEvents | 200;\n        var event \u003d {\n            name: name,\n            dates: []\n        };\n        // add up to 200 events\n        var max \u003d  Math.floor(Math.random() * maxNbEvents);\n        for (var j \u003d 0; j \u003c max; j++) {\n            var time \u003d (Math.random() * (endTime - startTime)) + startTime;\n            event.dates.push(new Date(time));\n        }\n        return event;\n    }\n\n    for (var i \u003d 0; i \u003c 10; i++) {\n        data.push(createEvent(names[i]));\n    }\n\n    var color \u003d d3.scale.category20();\n\n    var eventDropsChart \u003d d3.chart.eventDrops()\n        .eventLineColor(function (datum, index) {\n            return color(index);\n        })\n        .start(new Date(startTime))\n        .end(new Date(endTime))\n        .hasTopAxis(false)\n        .hasBottomAxis(false);\n\n    d3.select(\u0027#chart_placeholder\u0027)\n      .datum(data)\n      .call(eventDropsChart);\n\n});\n\u003c/script\u003e\n"
      },
      "dateCreated": "Jan 25, 2017 7:45:05 AM",
      "dateStarted": "Jan 26, 2017 1:20:47 AM",
      "dateFinished": "Jan 26, 2017 1:20:53 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Convert",
  "id": "2C847RCD9",
  "angularObjects": {
    "2C8HHURCR:shared_process": [],
    "2C8NZDHJ5:shared_process": [],
    "2C5FTUSC4:shared_process": [],
    "2C8Z7PJTV:shared_process": [],
    "2C6F57Q8V:shared_process": [],
    "2C8XRF9RX:shared_process": [],
    "2C7CYJ129:shared_process": [],
    "2C6EVCBVU:shared_process": [],
    "2C8Q1YWC3:shared_process": [],
    "2C5YW56B4:shared_process": [],
    "2C5V72FR6:shared_process": [],
    "2C7JSSPM2:shared_process": [],
    "2C5M2RA8J:shared_process": [],
    "2C5J6E164:shared_process": [],
    "2C9D71WWC:shared_process": [],
    "2C7WUM3K8:shared_process": [],
    "2C86612AJ:shared_process": [],
    "2C88D5QB1:shared_process": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}